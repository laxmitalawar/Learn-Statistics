{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  scipy.stats                     as  stats\n",
    "import  statsmodels.stats.proportion    as  SMP \n",
    "import  numpy                           as  np\n",
    "import  pandas                          as  pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "from IPython.display import Image\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does P-value signify about the statistical data?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.P-value is the conditional probability of observing the test statistic value or extreme than the sample result \n",
    "when the null hypothesis is true.\n",
    "2.P_value in statistics also states that conditional probabilty that NULL hypothesis being true.\n",
    "3.as p value increase chances of rejecting null hypothesis will be less or \n",
    "4.for p value >0.05 we are fail to reject the null hypothysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The mass of N1=20 acorns from oak trees up wind from a coal power plant and N2=30 acorns from oak trees downwind from the same coal power plant are measured. Are the acorns from trees downwind less massive then the ones from up wind? The sample sizes are not equal but we will assume that the population variance of sample 1 and sample 2 are equal. alpha = 0.05, t-critical for specified alpha level = -1.677  Samples are given below. State the Null and Alternate hypothesis. Which test would be used here?  Do you reject or retain the H0 based on your test? Find the t-statistic, Confidence Interval and R Square value. (10 MARKS) \n",
    " SAMPLE UP WIND: x1 = [10.8, 10.0, 8.2, 9.9, 11.6, 10.1, 11.3, 10.3, 10.7, 9.7, 7.8, 9.6, 9.7, 11.6, 10.3,  9.8, 12.3, 11.0, 10.4, 10.4] \n",
    " SAMPLE DOWNWIND: x2 = [7.8, 7.5, 9.5, 11.7, 8.1, 8.8, 8.8, 7.7, 9.7, 7.0, 9.0, 9.7, 11.3, 8.7, 8.8, 10.9,  10.3, 9.6, 8.4, 6.6, 7.2, 7.6, 11.5, 6.6, 8.6, 10.5, 8.4, 8.5, 10.2, 9.2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 =np.array( [10.8, 10.0, 8.2, 9.9, 11.6, 10.1, 11.3, 10.3, 10.7, 9.7, 7.8, 9.6, 9.7, 11.6, 10.3,  9.8, 12.3, 11.0, 10.4, 10.4] )\n",
    "x2=np.array([7.8, 7.5, 9.5, 11.7, 8.1, 8.8, 8.8, 7.7, 9.7, 7.0, 9.0, 9.7, 11.3, 8.7, 8.8, 10.9,  10.3, 9.6, 8.4, 6.6, 7.2, 7.6, 11.5, 6.6, 8.6, 10.5, 8.4, 8.5, 10.2, 9.2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis Test Statement\n",
    "\n",
    "Null Hypothesis (H0) : acorns from the downwind equally as massive than acorns from the upwind(downwind=upwind)\n",
    "\n",
    "Alternate Hypothesis (Ha) : acorns from the downwind either less or more massive than acorns from the upwind(downwind!=upwind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the samples are not paired, we use two sample independent t-test.\n",
    "\n",
    "**Testing the assumptions:**\n",
    "    1. Random sampling\n",
    "    2. Normality of continuous variable\n",
    "    3. Variance of stress levels across age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.05\n",
    "\n",
    "N1 = len(x1)\n",
    "N2 = len(x2)\n",
    "d1 = N1-1\n",
    "d2 = N2-1\n",
    "df = d1+d2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = np.std(x1,ddof=1)\n",
    "s2 = np.std(x2,ddof=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_bar = np.mean(x1)\n",
    "x2_bar = np.mean(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic -3.5981947686898033\n"
     ]
    }
   ],
   "source": [
    "sp = np.sqrt((d1*s1**2 + d2*s2**2)/df)\n",
    "se = sp*np.sqrt(1/N1 + 1/N2)\n",
    "t = (x2_bar - x1_bar)/(se)\n",
    "print(\"t-statistic\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  -3.5981947686898033 , p_twosided =  0.0007560337478801464 , p_onesided = 0.0003780168739400732\n"
     ]
    }
   ],
   "source": [
    "t, p_twosided = stats.ttest_ind(x2, x1, equal_var=True)\n",
    "print(\"t = \",t, \", p_twosided = \", p_twosided, \", p_onesided =\", p_twosided/2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For alpha = 0.05, critical value = 1.96\n",
    "p-value < 0.05 and absolute value of t_statistic = 3.6 which is greater than 1.96 , we can reject the null hypothesis\n",
    "acorns from the downwind either less or more massive than acorns from the upwind(downwind!=upwind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.275000000000002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.94"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since mean weight of acorns upwind > mean weight of acorns downwind thus we conclude that the acorns upwind are more massive than the acorns downwind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.207989550048865, 10.34201044995114)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.norm.interval(0.05,x1.mean(),np.std(x1,ddof = 1)) # CI for unpwind acorns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.851631686346463, 9.028368313653536)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.norm.interval(0.05,x2.mean(),np.std(x2,ddof = 1)) # CI for downwind acorns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.calculate R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'x1':[10.8, 10.0, 8.2, 9.9, 11.6, 10.1, 11.3, 10.3, 10.7, 9.7, 7.8, 9.6, 9.7, 11.6, 10.3,  9.8, 12.3, 11.0, 10.4, 10.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],'x2':[7.8, 7.5, 9.5, 11.7, 8.1, 8.8, 8.8, 7.7, 9.7, 7.0, 9.0, 9.7, 11.3, 8.7, 8.8, 10.9,  10.3, 9.6, 8.4, 6.6, 7.2, 7.6, 11.5, 6.6, 8.6, 10.5, 8.4, 8.5, 10.2, 9.2] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api         as     sm\n",
    "from   statsmodels.formula.api import ols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'x2 ~ x1 '\n",
    "model = ols(formula,data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             sum_sq    df         F    PR(>F)\n",
      "x1         0.103844   1.0  0.050578  0.823694\n",
      "Residual  57.488156  28.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "aov_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(aov_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>x2</td>        <th>  R-squared:         </th> <td>   0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td> 0.05058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 26 Feb 2020</td> <th>  Prob (F-statistic):</th>  <td> 0.824</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:54:45</td>     <th>  Log-Likelihood:    </th> <td> -52.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    30</td>      <th>  AIC:               </th> <td>   108.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    28</td>      <th>  BIC:               </th> <td>   111.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    8.8580</td> <td>    0.449</td> <td>   19.747</td> <td> 0.000</td> <td>    7.939</td> <td>    9.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>        <td>    0.0120</td> <td>    0.053</td> <td>    0.225</td> <td> 0.824</td> <td>   -0.097</td> <td>    0.121</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.976</td> <th>  Durbin-Watson:     </th> <td>   1.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.614</td> <th>  Jarque-Bera (JB):  </th> <td>   0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.266</td> <th>  Prob(JB):          </th> <td>   0.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.311</td> <th>  Cond. No.          </th> <td>    14.6</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     x2   R-squared:                       0.002\n",
       "Model:                            OLS   Adj. R-squared:                 -0.034\n",
       "Method:                 Least Squares   F-statistic:                   0.05058\n",
       "Date:                Wed, 26 Feb 2020   Prob (F-statistic):              0.824\n",
       "Time:                        11:54:45   Log-Likelihood:                -52.324\n",
       "No. Observations:                  30   AIC:                             108.6\n",
       "Df Residuals:                      28   BIC:                             111.5\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      8.8580      0.449     19.747      0.000       7.939       9.777\n",
       "x1             0.0120      0.053      0.225      0.824      -0.097       0.121\n",
       "==============================================================================\n",
       "Omnibus:                        0.976   Durbin-Watson:                   1.996\n",
       "Prob(Omnibus):                  0.614   Jarque-Bera (JB):                0.946\n",
       "Skew:                           0.266   Prob(JB):                        0.623\n",
       "Kurtosis:                       2.311   Cond. No.                         14.6\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above model we obtained R-square value =0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A machine is supposed to run for 300 minutes at a go, as told by a company on one unit of regular gas. A random sample of 50 machines is tested. The machine run for an average of 295 minutes, with a standard deviation of 20 minutes. \n",
    "Check the hypothesis if the mean run-time of a machine is 300 minutes or not. (15 MARKS)  \n",
    "a. Use a 0.05 level of significance. What is the region of acceptance? (5 marks) \n",
    "b. What hypothesis test would you choose to do for this problem and why? (5 marks) \n",
    "c. Would you reject or fail to reject the null hypothesis? (5 marks)  \n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "H0 = mean run time of muchine equals to 300 minutes.\n",
    "H1= mean run time of muchine is differing from 300 minutes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a. Use a 0.05 level of significance. What is the region of acceptance? (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the  ùõº , the level of significance according to the relative importance of the risks of committing Type I and Type II errors in the problem.\n",
    "\n",
    "In this example, making a Type I error means that you conclude that the population mean is not 300 minutes when it is 300 minutes. This implies that you will take corrective action, even though the process is working well (false alarm).\n",
    "\n",
    "On the other hand, when the population mean is less than 300 minutes and you conclude that the population mean is 300 minutes, you commit a Type II error. Here, you allow the process to continue without adjustment, even though an adjustment is needed (missed opportunity).\n",
    "\n",
    "Here, we select  ùõº  = 0.05 and n, sample size = 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We know the ùõº is 0.05. So, the critical values of the ùëçùëÜùëáùê¥ùëá test statistic are -1.96 and 1.96.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.96\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(round(stats.norm.isf(q = 0.025),2))) # Here we use alpha by 2  for two-tailed test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#sample size\n",
    "n=50\n",
    "#std of standard devition\n",
    "std=20\n",
    "# sample mean\n",
    "mean=295\n",
    "alpha=0.05\n",
    "#population mean\n",
    "mean_cliam=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Rejection region is $Z_{STAT}$ < -1.96 or $Z_{STAT}$ > 1.96\n",
    "* ### Acceptance or non-rejection regions is -1.96 $\\leq$ $Z_{STAT}$ $\\leq$ 1.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What hypothesis test would you choose to do for this problem and why? (5 marks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from data we have sample size>30, we can go for statistical test or p value approch which is more significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of Z observed is -1.76777\n"
     ]
    }
   ],
   "source": [
    "XAvg  = 295\n",
    "mu    = 300\n",
    "sigma = 20\n",
    "n     = 50\n",
    "Z = (XAvg - mu)/(sigma/np.sqrt(n))\n",
    "print('Value of Z observed is %2.5f' %Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard error\t: 2.82842712474619\n",
      "t-statistic\t: -1.7677669529663689\n",
      "Critical region: (-0.5099217500020995, 0.5099217500020995)\n"
     ]
    }
   ],
   "source": [
    "# population mean\n",
    "mu = 300\n",
    "# sample mean\n",
    "xbar = 295\n",
    "# sample SD\n",
    "s = 20\n",
    "# sample size\n",
    "n = 50\n",
    "# significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# std. error\n",
    "se = s / np.sqrt(n)\n",
    "print(\"Standard error\\t:\", se)\n",
    "\n",
    "# cal t-statistic\n",
    "tstat = (xbar - mu) / se\n",
    "print(\"t-statistic\\t:\", tstat)\n",
    "\n",
    "# cal critical region\n",
    "tcritical = stats.t.cdf(alpha/2, n-1) # two tail\n",
    "print(\"Critical region:\", (-tcritical, tcritical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value\t\t: 0.04166307987075465\n"
     ]
    }
   ],
   "source": [
    "# cal p-value\n",
    "pvalue = stats.t.cdf(tstat, n-1)\n",
    "print(\"p-value\\t\\t:\", pvalue)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "c. Would you reject or fail to reject the null hypothesis? (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P_value is less than 0.05 , reject the null hypothysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hence there is chance that mean run time of muchine is differing from 300 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
